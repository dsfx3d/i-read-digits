{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.initializers import he_uniform\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Lambda, Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed=11\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Fetched datasets from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../datasets/'\n",
    "augmentation_path = os.path.join(dataset_path, 'augmentations')\n",
    "\n",
    "train_path = os.path.join(dataset_path, 'train.csv')\n",
    "test_path = os.path.join(dataset_path, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_path)\n",
    "eval_data = pd.read_csv(test_path)\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Transformed datasets to deep learning friendly format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_data.values.astype(np.float32)\n",
    "eval_set = eval_data.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Created dev and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(random_state=seed, test_size=.3, n_splits=1)\n",
    "\n",
    "for train_index, test_index in split.split(train_set[:,1:], train_set[:,0]):\n",
    "    test_set = train_set[test_index]\n",
    "    train_set = train_set[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(random_state=seed, test_size=.5, n_splits=1)\n",
    "\n",
    "for dev_index, test_index in split.split(test_set[:,1:], test_set[:,0]):\n",
    "    dev_set = test_set[dev_index]\n",
    "    test_set = test_set[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:\t (29400, 785)\n",
      "test set:\t (6300, 785)\n",
      "dev set:\t (6300, 785)\n"
     ]
    }
   ],
   "source": [
    "print('train set:\\t', train_set.shape)\n",
    "print('test set:\\t', test_set.shape)\n",
    "print('dev set:\\t', dev_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 removed intersecting instances b/w train, test and dev sets. so the net never learn from test and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_set, columns=train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Notes__:\n",
    " \n",
    " 1. Successful set up of dev and test set.\n",
    " 2. Training set will be extended using augmented data \n",
    " 3. Training Dev set will be created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Shifted Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1 Fetched shifted augmentations from local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_class_every_aug_filepath = os.path.join(augmentation_path, 'class-every.csv')\n",
    "\n",
    "shifted_class_every_aug_data = pd.read_csv(shifted_class_every_aug_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Appended shifted augmentation to train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.append(shifted_class_every_aug_data)\n",
    "train_set = train_data.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Created train and train-dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(random_state=seed, test_size=.12, n_splits=1)\n",
    "\n",
    "for train_index, train_dev_index in split.split(train_set[:,1:], train_set[:,0]):\n",
    "    train_dev_set = train_set[train_dev_index]\n",
    "    train_set = train_set[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Set</th>\n",
       "      <th>Size</th>\n",
       "      <th>Perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training set</td>\n",
       "      <td>141215</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training dev set</td>\n",
       "      <td>19257</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev set</td>\n",
       "      <td>6300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test set</td>\n",
       "      <td>6300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>total</td>\n",
       "      <td>173072</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Set    Size Perc\n",
       "0      training set  141215   82\n",
       "1  training dev set   19257   11\n",
       "2           dev set    6300    4\n",
       "3          test set    6300    4\n",
       "4             total  173072  100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(train_set)\n",
    "train_dev_size = len(train_dev_set)\n",
    "dev_size = len(dev_set)\n",
    "test_size = len(test_set)\n",
    "\n",
    "total_size = train_dev_size+train_size+dev_size+test_size\n",
    "\n",
    "labels = np.array(['training set','training dev set','dev set','test set','total']).reshape(-1,1)\n",
    "sizes = np.array([train_size, train_dev_size, dev_size, test_size, total_size]).reshape(-1,1)\n",
    "perc = (np.round((sizes/total_size).reshape(-1,1), decimals=2)*100).astype(np.int32)\n",
    "\n",
    "pd.DataFrame(np.c_[labels, sizes, perc], columns=['Set','Size','Perc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preproccessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Multiple approaches for normalization can be used:__\n",
    "\n",
    "1. Min-Max Scaling, mostly (0,1)\n",
    "   \n",
    "2. Standardize pixel distribution to ensure zero mean and one variance\n",
    "\n",
    "__Tip: scale data to activation function's range__\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(set_,mn=0,mx=255):\n",
    "    return (set_-mn)/(mx-mn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[:,1:] = min_max_scale(train_set[:,1:])\n",
    "train_dev_set[:,1:] = min_max_scale(train_dev_set[:,1:])\n",
    "dev_set[:,1:] = min_max_scale(dev_set[:,1:])\n",
    "test_set[:,1:] = min_max_scale(test_set[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = min_max_scale(eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving in payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_path = '../payload'\n",
    "scaled_path = 'mnmxs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(payload_path, 'normal-payload.npz')\n",
    "\n",
    "np.savez(filepath, train_set=train_set, test_set=test_set, dev_set=dev_set, train_dev_set=train_dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(payload_path, 'normal-eval-payload.npz')\n",
    "np.savez(filepath, eval_set=eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
