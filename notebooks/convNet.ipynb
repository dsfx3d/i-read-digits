{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.initializers import he_uniform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed=11\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_predictions(X, y):\n",
    "    I = np.random.permutation(X.shape[0])[:3]\n",
    "\n",
    "    for c, i in enumerate(I):\n",
    "        img = X[i].reshape(28, 28)\n",
    "        plt.subplot(131+c)\n",
    "        plt.imshow(img, cmap=plt.cm.gray)\n",
    "        plt.title(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_history(history):\n",
    "    acc_list = history.history['acc']\n",
    "    acc = acc_list[-1]\n",
    "    best_acc_index = np.array(acc_list).argmax()\n",
    "    best_acc = acc_list[best_acc_index]\n",
    "    \n",
    "    print('Accuracy: {:.4f} \\tBest Accuracy: {:.4f} \\t\\t@ {} epoch'.format(acc, best_acc, best_acc_index+1))\n",
    "    \n",
    "    if 'val_acc' in  history.history.keys():\n",
    "        val_acc_list = history.history['val_acc']\n",
    "        val_acc = val_acc_list[-1]\n",
    "        best_val_acc_index = np.array(val_acc_list).argmax()\n",
    "        best_val_acc = val_acc_list[best_val_acc_index]\n",
    "        print('Dev Accuracy: {:.4f} \\tBest Dev Accuracy: {:.4f} \\t@ {} epoch'.format(val_acc, best_val_acc, best_val_acc_index+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15,4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['acc'], label='Training Set')\n",
    "    \n",
    "    if 'val_acc' in history.history.keys():\n",
    "        plt.plot(history.history['val_acc'], label='Validation Set')\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'], label='Training Set')\n",
    "    \n",
    "    if 'val_loss' in history.history.keys():\n",
    "        plt.plot(history.history['val_loss'], label='Validation Set')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_matrix(cm):\n",
    "    row_sum = cm.sum(axis=1, keepdims=True)\n",
    "    norm_cm = cm/row_sum\n",
    "    \n",
    "    np.fill_diagonal(norm_cm, 0)\n",
    "    sns.heatmap(norm_cm, robust=True, fmt=\"f\", cmap='RdBu_r', vmin=0, vmax=4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_dir = '../payload/'\n",
    "payload_file = 'normal-payload.npz'\n",
    "\n",
    "payload_path = os.path.join(payload_dir, payload_file)\n",
    "\n",
    "payload_archive = np.load(payload_path)\n",
    "\n",
    "dev_set = payload_archive['dev_set']\n",
    "test_set = payload_archive['test_set']\n",
    "train_dev_set = payload_archive['train_dev_set']\n",
    "train_set = payload_archive['train_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = train_set[:,1:].reshape(-1,28,28,1), train_set[:,0]\n",
    "trainY = np_utils.to_categorical(trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_devX, train_devy = train_dev_set[:,1:].reshape(-1,28,28,1), train_dev_set[:,0]\n",
    "train_devY = np_utils.to_categorical(train_devy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testy = test_set[:,1:].reshape(-1,28,28,1), test_set[:,0]\n",
    "testY = np_utils.to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX, devy = dev_set[:,1:].reshape(-1,28,28,1), dev_set[:,0]\n",
    "devY = np_utils.to_categorical(devy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = .4\n",
    "lambd = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Optimization Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-2\n",
    "lr_decay = 9e-3\n",
    "batch_size=128\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "models_dir = '../models'\n",
    "modelpath = os.path.join(models_dir,'ConvNet.{epoch:02d}-{val_loss:.2f}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2 Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sched(epoch):\n",
    "    return 1/(1+lr_decay*epoch) * alpha#*(lr_decay**epoch)\n",
    "\n",
    "schd = LearningRateScheduler(lr_sched, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(filepath=modelpath, save_best_only=True, verbose=1, monitor='val_acc')\n",
    "tfboard = TensorBoard(log_dir='./logs', batch_size=128, write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "callbacks = [schd, early_stopping, model_checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3 The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None#'ConvNet.13-0.03.hdf5'\n",
    "\n",
    "if model:\n",
    "    alpha = 0.008952551477170993\n",
    "    model = os.path.join('../models', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path=None):        \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if path:\n",
    "        return load_model(path)\n",
    "    \n",
    "    model.add(Conv2D(64,(3,3), strides=2, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "    model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # layer 1\n",
    "    model.add(\n",
    "        Dense(1024, \n",
    "        bias_initializer='zeros', kernel_initializer=he_uniform(seed), \n",
    "        kernel_regularizer=l2(lambd),\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    # layer 1\n",
    "    model.add(\n",
    "        Dense(1024, \n",
    "        bias_initializer='zeros', kernel_initializer=he_uniform(seed), \n",
    "        kernel_regularizer=l2(lambd),\n",
    "        )\n",
    "    )\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    adam_optimizer = Adam(lr=alpha)\n",
    "    model.compile(optimizer=adam_optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.4 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127093 samples, validate on 14122 samples\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "127093/127093 [==============================] - 119s 934us/step - loss: 0.4863 - acc: 0.8490 - val_loss: 0.1499 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.95249, saving model to ../models/ConvNet.01-0.15.hdf5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009910802775024779.\n",
      "127093/127093 [==============================] - 121s 954us/step - loss: 0.2480 - acc: 0.9238 - val_loss: 0.1415 - val_acc: 0.9577\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.95249 to 0.95773, saving model to ../models/ConvNet.02-0.14.hdf5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.009823182711198428.\n",
      "127093/127093 [==============================] - 125s 985us/step - loss: 0.2193 - acc: 0.9317 - val_loss: 0.1302 - val_acc: 0.9620\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.95773 to 0.96205, saving model to ../models/ConvNet.03-0.13.hdf5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.009737098344693282.\n",
      "127093/127093 [==============================] - 119s 940us/step - loss: 0.2043 - acc: 0.9364 - val_loss: 0.1355 - val_acc: 0.9569\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.009652509652509652.\n",
      "127093/127093 [==============================] - 120s 943us/step - loss: 0.1901 - acc: 0.9413 - val_loss: 0.1010 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96205 to 0.96870, saving model to ../models/ConvNet.05-0.10.hdf5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009569377990430623.\n",
      "127093/127093 [==============================] - 123s 965us/step - loss: 0.1784 - acc: 0.9447 - val_loss: 0.1041 - val_acc: 0.9693\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.96870 to 0.96934, saving model to ../models/ConvNet.06-0.10.hdf5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009487666034155597.\n",
      "127093/127093 [==============================] - 121s 948us/step - loss: 0.1742 - acc: 0.9461 - val_loss: 0.1384 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.009407337723424272.\n",
      "127093/127093 [==============================] - 123s 964us/step - loss: 0.1622 - acc: 0.9502 - val_loss: 0.1133 - val_acc: 0.9653\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.009328358208955223.\n",
      "127093/127093 [==============================] - 118s 928us/step - loss: 0.1578 - acc: 0.9509 - val_loss: 0.1285 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.009250693802035153.\n",
      "127093/127093 [==============================] - 122s 957us/step - loss: 0.1534 - acc: 0.9529 - val_loss: 0.0964 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.96934 to 0.97118, saving model to ../models/ConvNet.10-0.10.hdf5\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.009174311926605503.\n",
      "127093/127093 [==============================] - 121s 953us/step - loss: 0.1488 - acc: 0.9539 - val_loss: 0.0959 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.009099181073703368.\n",
      "127093/127093 [==============================] - 130s 1ms/step - loss: 0.1459 - acc: 0.9545 - val_loss: 0.1014 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.009025270758122744.\n",
      "127093/127093 [==============================] - 131s 1ms/step - loss: 0.1398 - acc: 0.9572 - val_loss: 0.0948 - val_acc: 0.9717\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.97118 to 0.97168, saving model to ../models/ConvNet.13-0.09.hdf5\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.008952551477170993.\n",
      "127093/127093 [==============================] - 116s 914us/step - loss: 0.1372 - acc: 0.9579 - val_loss: 0.1009 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.008880994671403198.\n",
      "127093/127093 [==============================] - 112s 880us/step - loss: 0.1369 - acc: 0.9582 - val_loss: 0.1013 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.008810572687224669.\n",
      "127093/127093 [==============================] - 122s 958us/step - loss: 0.1320 - acc: 0.9592 - val_loss: 0.0945 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.97168 to 0.97231, saving model to ../models/ConvNet.16-0.09.hdf5\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.008741258741258742.\n",
      "127093/127093 [==============================] - 121s 951us/step - loss: 0.1306 - acc: 0.9599 - val_loss: 0.1097 - val_acc: 0.9673\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.008673026886383347.\n",
      "127093/127093 [==============================] - 119s 936us/step - loss: 0.1264 - acc: 0.9605 - val_loss: 0.0902 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.97231 to 0.97245, saving model to ../models/ConvNet.18-0.09.hdf5\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.008605851979345956.\n",
      "127093/127093 [==============================] - 114s 895us/step - loss: 0.1270 - acc: 0.9606 - val_loss: 0.0886 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97245 to 0.97437, saving model to ../models/ConvNet.19-0.09.hdf5\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.008539709649871904.\n",
      "127093/127093 [==============================] - 111s 871us/step - loss: 0.1243 - acc: 0.9610 - val_loss: 0.0835 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.97437 to 0.97472, saving model to ../models/ConvNet.20-0.08.hdf5\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.008474576271186442.\n",
      "127093/127093 [==============================] - 113s 891us/step - loss: 0.1222 - acc: 0.9623 - val_loss: 0.0879 - val_acc: 0.9736\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.008410428931875526.\n",
      "127093/127093 [==============================] - 115s 903us/step - loss: 0.1202 - acc: 0.9626 - val_loss: 0.0773 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.97472 to 0.97635, saving model to ../models/ConvNet.22-0.08.hdf5\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.008347245409015025.\n",
      "127093/127093 [==============================] - 100s 784us/step - loss: 0.1193 - acc: 0.9630 - val_loss: 0.0956 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.008285004142502071.\n",
      "127093/127093 [==============================] - 101s 792us/step - loss: 0.1189 - acc: 0.9628 - val_loss: 0.0889 - val_acc: 0.9715\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.008223684210526317.\n",
      "127093/127093 [==============================] - 98s 773us/step - loss: 0.1152 - acc: 0.9640 - val_loss: 0.0782 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.97635 to 0.97642, saving model to ../models/ConvNet.25-0.08.hdf5\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.008163265306122448.\n",
      "127093/127093 [==============================] - 99s 777us/step - loss: 0.1165 - acc: 0.9638 - val_loss: 0.0938 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.008103727714748784.\n",
      "127093/127093 [==============================] - 101s 795us/step - loss: 0.1129 - acc: 0.9649 - val_loss: 0.0888 - val_acc: 0.9730\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.008045052292839904.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127093/127093 [==============================] - 102s 799us/step - loss: 0.1112 - acc: 0.9658 - val_loss: 0.0824 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.007987220447284346.\n",
      "127093/127093 [==============================] - 100s 788us/step - loss: 0.1098 - acc: 0.9652 - val_loss: 0.0807 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.007930214115781127.\n",
      "127093/127093 [==============================] - 99s 776us/step - loss: 0.1085 - acc: 0.9664 - val_loss: 0.0857 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.007874015748031496.\n",
      "127093/127093 [==============================] - 101s 792us/step - loss: 0.1086 - acc: 0.9660 - val_loss: 0.0948 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.007818608287724786.\n",
      "127093/127093 [==============================] - 98s 775us/step - loss: 0.1080 - acc: 0.9667 - val_loss: 0.0837 - val_acc: 0.9744\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(trainX, trainY, batch_size=batch_size, epochs=num_epoch, verbose=1, \n",
    "                  callbacks=callbacks, validation_split=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3 Best Model Yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-655916e35ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "print_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
