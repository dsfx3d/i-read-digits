{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.9.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7cc2c9268bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhe_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.initializers import he_uniform\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed=11\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_predictions(X, y):\n",
    "    I = np.random.permutation(X.shape[0])[:3]\n",
    "\n",
    "    for c, i in enumerate(I):\n",
    "        img = X[i].reshape(28, 28)\n",
    "        plt.subplot(131+c)\n",
    "        plt.imshow(img, cmap=plt.cm.gray)\n",
    "        plt.title(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_history(history):\n",
    "    acc_list = history.history['acc']\n",
    "    acc = acc_list[-1]\n",
    "    best_acc_index = np.array(acc_list).argmax()\n",
    "    best_acc = acc_list[best_acc_index]\n",
    "    \n",
    "    print('Accuracy: {:.4f} \\tBest Accuracy: {:.4f} \\t\\t@ {} epoch'.format(acc, best_acc, best_acc_index+1))\n",
    "    \n",
    "    if 'val_acc' in  history.history.keys():\n",
    "        val_acc_list = history.history['val_acc']\n",
    "        val_acc = val_acc_list[-1]\n",
    "        best_val_acc_index = np.array(val_acc_list).argmax()\n",
    "        best_val_acc = val_acc_list[best_val_acc_index]\n",
    "        print('Dev Accuracy: {:.4f} \\tBest Dev Accuracy: {:.4f} \\t@ {} epoch'.format(val_acc, best_val_acc, best_val_acc_index+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15,4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['acc'], label='Training Set')\n",
    "    \n",
    "    if 'val_acc' in history.history.keys():\n",
    "        plt.plot(history.history['val_acc'], label='Validation Set')\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'], label='Training Set')\n",
    "    \n",
    "    if 'val_loss' in history.history.keys():\n",
    "        plt.plot(history.history['val_loss'], label='Validation Set')\n",
    "    plt.title('Loss vs Epochs')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_matrix(cm):\n",
    "    row_sum = cm.sum(axis=1, keepdims=True)\n",
    "    norm_cm = cm/row_sum\n",
    "    \n",
    "    np.fill_diagonal(norm_cm, 0)\n",
    "    sns.heatmap(norm_cm, robust=True, fmt=\"f\", cmap='RdBu_r', vmin=0, vmax=4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_dir = '../payload/'\n",
    "payload_file = 'normal-payload.npz'\n",
    "\n",
    "payload_path = os.path.join(payload_dir, payload_file)\n",
    "\n",
    "payload_archive = np.load(payload_path)\n",
    "\n",
    "dev_set = payload_archive['dev_set']\n",
    "test_set = payload_archive['test_set']\n",
    "train_dev_set = payload_archive['train_dev_set']\n",
    "train_set = payload_archive['train_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainy = train_set[:,1:], train_set[:,0]\n",
    "trainY = np_utils.to_categorical(trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_devX, train_devy = train_dev_set[:,1:], train_dev_set[:,0]\n",
    "train_devY = np_utils.to_categorical(train_devy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testy = test_set[:,1:], test_set[:,0]\n",
    "testY = np_utils.to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX, devy = dev_set[:,1:], dev_set[:,0]\n",
    "devY = np_utils.to_categorical(devy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.4\n",
    "lambd = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(path=None):        \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if path:\n",
    "        return load_model(path)\n",
    "    \n",
    "    # layer 1\n",
    "    model.add(\n",
    "        Dense(1024, \n",
    "        bias_initializer='zeros', kernel_initializer=he_uniform(seed), \n",
    "        kernel_regularizer=l2(lambd),\n",
    "        input_dim=784)\n",
    "    )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "        # layer 1\n",
    "    model.add(\n",
    "        Dense(512, \n",
    "        bias_initializer='zeros', kernel_initializer=he_uniform(seed), \n",
    "        kernel_regularizer=l2(lambd),\n",
    "        input_dim=784)\n",
    "    )\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None#'MLPNet.44-0.09.hdf5'\n",
    "\n",
    "if model:\n",
    "    #alpha = 0.004181203352191771\n",
    "    model = os.path.join('../models', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Optimization Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-3\n",
    "lr_decay = 1e-2\n",
    "batch_size=128\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "models_dir = '../models'\n",
    "modelpath = os.path.join(models_dir,'MLPNet.{epoch:02d}-{val_loss:.2f}.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2 Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_sched(epoch):\n",
    "    return 1/(1+lr_decay*epoch) * alpha#*(lr_decay**epoch)\n",
    "\n",
    "schd = LearningRateScheduler(lr_sched, verbose=1)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(filepath=modelpath, save_best_only=True, verbose=1, monitor='val_acc')\n",
    "\n",
    "callbacks = [schd, early_stopping, model_checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3 Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = Adam(lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127093 samples, validate on 14122 samples\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "127093/127093 [==============================] - 83s 650us/step - loss: 0.4975 - acc: 0.8394 - val_loss: 0.2140 - val_acc: 0.9340\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.93400, saving model to ../models/MLPNet.01-0.21.hdf5\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0009900990099009901.\n",
      "127093/127093 [==============================] - 86s 676us/step - loss: 0.2487 - acc: 0.9220 - val_loss: 0.1573 - val_acc: 0.9482\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.93400 to 0.94817, saving model to ../models/MLPNet.02-0.16.hdf5\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.000980392156862745.\n",
      "127093/127093 [==============================] - 83s 651us/step - loss: 0.1936 - acc: 0.9380 - val_loss: 0.1301 - val_acc: 0.9601\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.94817 to 0.96006, saving model to ../models/MLPNet.03-0.13.hdf5\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0009708737864077671.\n",
      "127093/127093 [==============================] - 82s 646us/step - loss: 0.1643 - acc: 0.9473 - val_loss: 0.1314 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0009615384615384615.\n",
      "127093/127093 [==============================] - 80s 626us/step - loss: 0.1420 - acc: 0.9546 - val_loss: 0.1194 - val_acc: 0.9635\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.96006 to 0.96346, saving model to ../models/MLPNet.05-0.12.hdf5\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0009523809523809524.\n",
      "127093/127093 [==============================] - 78s 616us/step - loss: 0.1280 - acc: 0.9587 - val_loss: 0.1008 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.96346 to 0.96856, saving model to ../models/MLPNet.06-0.10.hdf5\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0009433962264150942.\n",
      "127093/127093 [==============================] - 81s 638us/step - loss: 0.1139 - acc: 0.9630 - val_loss: 0.1063 - val_acc: 0.9659\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0009345794392523365.\n",
      "127093/127093 [==============================] - 80s 628us/step - loss: 0.1047 - acc: 0.9663 - val_loss: 0.0878 - val_acc: 0.9735\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.96856 to 0.97352, saving model to ../models/MLPNet.08-0.09.hdf5\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0009259259259259259.\n",
      "127093/127093 [==============================] - 83s 656us/step - loss: 0.0968 - acc: 0.9683 - val_loss: 0.0869 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.97352 to 0.97394, saving model to ../models/MLPNet.09-0.09.hdf5\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0009174311926605504.\n",
      "127093/127093 [==============================] - 85s 671us/step - loss: 0.0896 - acc: 0.9704 - val_loss: 0.0858 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0009090909090909091.\n",
      "127093/127093 [==============================] - 78s 610us/step - loss: 0.0828 - acc: 0.9730 - val_loss: 0.0827 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.97394 to 0.97472, saving model to ../models/MLPNet.11-0.08.hdf5\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0009009009009009008.\n",
      "127093/127093 [==============================] - 79s 623us/step - loss: 0.0770 - acc: 0.9751 - val_loss: 0.0807 - val_acc: 0.9747\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.97472 to 0.97472, saving model to ../models/MLPNet.12-0.08.hdf5\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0008928571428571428.\n",
      "127093/127093 [==============================] - 76s 600us/step - loss: 0.0720 - acc: 0.9760 - val_loss: 0.0814 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.97472 to 0.97557, saving model to ../models/MLPNet.13-0.08.hdf5\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0008849557522123895.\n",
      "127093/127093 [==============================] - 78s 614us/step - loss: 0.0671 - acc: 0.9774 - val_loss: 0.0779 - val_acc: 0.9767\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.97557 to 0.97670, saving model to ../models/MLPNet.14-0.08.hdf5\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0008771929824561404.\n",
      "127093/127093 [==============================] - 77s 605us/step - loss: 0.0648 - acc: 0.9786 - val_loss: 0.0803 - val_acc: 0.9739\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0008695652173913045.\n",
      "127093/127093 [==============================] - 71s 562us/step - loss: 0.0615 - acc: 0.9799 - val_loss: 0.0764 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0008620689655172415.\n",
      "127093/127093 [==============================] - 76s 598us/step - loss: 0.0583 - acc: 0.9808 - val_loss: 0.0791 - val_acc: 0.9759\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0008547008547008548.\n",
      "127093/127093 [==============================] - 75s 593us/step - loss: 0.0525 - acc: 0.9820 - val_loss: 0.0725 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.97670 to 0.97805, saving model to ../models/MLPNet.18-0.07.hdf5\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0008474576271186442.\n",
      "127093/127093 [==============================] - 76s 599us/step - loss: 0.0541 - acc: 0.9819 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.97805 to 0.97876, saving model to ../models/MLPNet.19-0.07.hdf5\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0008403361344537816.\n",
      "127093/127093 [==============================] - 78s 614us/step - loss: 0.0505 - acc: 0.9830 - val_loss: 0.0747 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0008333333333333334.\n",
      "127093/127093 [==============================] - 76s 595us/step - loss: 0.0478 - acc: 0.9833 - val_loss: 0.0728 - val_acc: 0.9785\n",
      "\n",
      "Epoch 00021: val_acc did not improve\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0008264462809917356.\n",
      "127093/127093 [==============================] - 77s 604us/step - loss: 0.0463 - acc: 0.9849 - val_loss: 0.0817 - val_acc: 0.9756\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.000819672131147541.\n",
      "127093/127093 [==============================] - 71s 556us/step - loss: 0.0455 - acc: 0.9846 - val_loss: 0.0778 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0008130081300813008.\n",
      "127093/127093 [==============================] - 69s 544us/step - loss: 0.0410 - acc: 0.9862 - val_loss: 0.0756 - val_acc: 0.9789\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.97876 to 0.97890, saving model to ../models/MLPNet.24-0.08.hdf5\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0008064516129032259.\n",
      "127093/127093 [==============================] - 68s 535us/step - loss: 0.0433 - acc: 0.9853 - val_loss: 0.0721 - val_acc: 0.9783\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0008.\n",
      "127093/127093 [==============================] - 66s 522us/step - loss: 0.0413 - acc: 0.9859 - val_loss: 0.0751 - val_acc: 0.9780\n",
      "\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0007936507936507937.\n",
      "127093/127093 [==============================] - 72s 566us/step - loss: 0.0389 - acc: 0.9864 - val_loss: 0.0734 - val_acc: 0.9779\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0007874015748031496.\n",
      "127093/127093 [==============================] - 67s 526us/step - loss: 0.0361 - acc: 0.9876 - val_loss: 0.0724 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "127093/127093 [==============================] - 68s 536us/step - loss: 0.0355 - acc: 0.9878 - val_loss: 0.0727 - val_acc: 0.9774\n",
      "\n",
      "Epoch 00029: val_acc did not improve\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=adam_optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "history=model.fit(trainX, trainY, batch_size=batch_size, epochs=num_epoch, verbose=1, \n",
    "                  callbacks=callbacks, validation_split=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3 Best Model Yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_error(X,y):\n",
    "    pred = model.predict_classes(X)\n",
    "    \n",
    "    cm=confusion_matrix(y_pred=pred, y_true=y)\n",
    "    row_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    err_matrix = cm/row_sum\n",
    "    \n",
    "    np.fill_diagonal(err_matrix, 0)\n",
    "    sns.heatmap(err_matrix, fmt=\"f\", vmin=0, vmax=err_matrix.max())\n",
    "\n",
    "    \n",
    "    return np.sum(pred!=y)/len(y)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('training set')\n",
    "train_err = pred_error(trainX, trainy)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('training-dev set')\n",
    "train_dev_err = pred_error(train_devX, train_devy)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('dev set')\n",
    "dev_err = pred_error(devX, devy)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('test set')\n",
    "test_err = pred_error(testX, testy)\n",
    "\n",
    "print('Human Level Error:\\t~{:.2f} %'.format(0.00))\n",
    "print('Training Error:\\t\\t {:.2f} %'.format(train_err))\n",
    "print('Training-Dev Error:\\t {:.2f} %'.format(train_dev_err))\n",
    "print('Dev Error:\\t\\t {:.2f} %'.format(dev_err))\n",
    "print('Test Error:\\t\\t {:.2f} %'.format(test_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Human Level Error:\\t~{:.2f} %'.format(0.00))\n",
    "print('Training Error:\\t\\t {:.2f} %'.format(train_err))\n",
    "print('Training-Dev Error:\\t {:.2f} %'.format(train_dev_err))\n",
    "print('Dev Error:\\t\\t {:.2f} %'.format(dev_err))\n",
    "print('Test Error:\\t\\t {:.2f} %'.format(test_err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
